{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-secret CIA document reveals the CIA had an eye for hackers who were willing to hack into the computer systems of government agencies. The documents, which were leaked to The Daily Beast, reveal that the CIA had close ties to Vladimir Putin.\n",
      "\n",
      "The CIA document released by WikiLeaks shows that the agency had close ties to hackers who were willing to hack into the computer systems of government agencies. Photo: AP\n",
      "\n",
      "The document, which is now under investigation by the Justice Department, shows the CIA had close ties to hackers who were willing to hack into the computer systems of government agencies. Photo: AP\n",
      "\n",
      "The document shows that the CIA had close ties to hackers who were willing to hack into the computer systems of government agencies. Photo: AP\n",
      "\n",
      "The document shows that the CIA had close ties to hackers who were willing to hack into the computer systems of government agencies. Photo: AP\n",
      "\n",
      "The CIA document shows that the CIA had close ties to hackers who were willing to hack into the computer systems of government agencies. Photo: AP\n",
      "\n",
      "The document shows that the CIA had close ties to hackers who were willing to hack into the computer systems of government agencies. Photo: AP\n",
      "\n",
      "The document shows that the CIA had close ties to hackers who were willing to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By Michael Lippman\n",
      "\n",
      "PBS NewsHour\n",
      "\n",
      "WASHINGTON, D.C. -- When the Republican National Committee held its \"America First\" conference in Washington, D.C., it didn't go far enough to acknowledge the Democratic party's record on the issue of climate change.\n",
      "\n",
      "The RNC held its 2015 conference in Cleveland but only after the DNC released its final report.\n",
      "\n",
      "The RNC announced last week that the party could no longer \"promote the party's\n",
      "==========\n",
      "I'll be getting into the game by the end of the week, but it's a good time to get your head around the current state of the game, so let's get started.\n",
      "\n",
      "The most important thing to get to grips with is the current state of your characters. This means that your character can change in ways that will make their character more interesting, more fun, more fun.\n",
      "\n",
      "The next part of my story will be about the most important things to have in your\n",
      "==========\n",
      ", which is the number of files that are available in the local directory (not in the local directory) to be searched for. The value of the 'default' option defaults to the file that is being searched.\n",
      "\n",
      "defaults to the file that is being searched. The value of the option defaults to the file that is being searched. The default options provide a way to specify different options in the config file.\n",
      "\n",
      "You can change the default options by using the'setdefaultoptions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mI believe in unicorns because\u001b[0m that's where they belong, in my book.\"\n",
      "\n",
      "It's a common refrain in the community. \"What is it about unicorns that makes them so great?\" I ask, though I can't be sure if it's just because they're so rare or if they're just, well, unicorns. \"I think they're good,\" I get back.\n",
      "\n",
      "But, as I say, it's not just because they're so rare. They're\n",
      "==========\n",
      "\u001b[1mI believe in unicorns because\u001b[0m I believe in unicorns.\"\n",
      "\n",
      "\"But I don't have a unicorn,\" Dr. Shumpert replied. \"I have a lot of unicorns.\"\n",
      "\n",
      "\"Well, you know what I mean?\" Dr. Shumpert asked.\n",
      "\n",
      "\"Yeah, I'm sure you've seen all of the unicorns,\" Dr. Shumpert added with a smile. \"But I'm not sure I'm the only one that has heard of unic\n",
      "==========\n",
      "\u001b[1mI believe in unicorns because\u001b[0m they are like magic. They are pure. They are the perfect symbol of good, love and harmony. And then there are other, less important symbols that are like magic. They are the foundation of all that we love.\n",
      "\n",
      "When I was young, I was taught by my teachers that there were two kinds of unicorns. A kind that were very good at communicating the meaning of a message. And a kind that were very cruel and unfair. And so I\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf3b493d0d34a58bc33c46dd7aae2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": 2,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "# Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model\n",
    "ai = aitextgen()\n",
    "\n",
    "ai.generate()\n",
    "ai.generate(n=3, max_length=100)\n",
    "ai.generate(n=3, prompt=\"I believe in unicorns because\", max_length=100)\n",
    "ai.generate_to_file(n=10, prompt=\"I believe in unicorns because\", max_length=100, temperature=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are tokenizing a CSV file, but you did not set line_by_line=True. Please change if unintended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d9f3074ae34f7893d34f8fba3755ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4961 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": 31,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3756b41a7b8c4ae6bdb6f377ab71640d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128055 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": 31,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c63c9cd6d81b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# On a 2020 8-core iMac, this took ~25 minutes to run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#ai.train(data, batch_size=8, num_steps=50000, generate_every=5000, save_every=5000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_by_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# Generate text from it!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#ai.generate(10, prompt=\"ROMEO:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/aitextgen/aitextgen.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, output_dir, fp16, fp16_opt_level, n_gpu, tpu_cores, max_grad_norm, gradient_accumulation_steps, seed, learning_rate, weight_decay, adam_epsilon, warmup_steps, num_steps, save_every, generate_every, n_generate, loggers, batch_size, num_workers, benchmark, avg_loss_smoothing, save_gdrive, run_id, progress_bar_refresh_rate, freeze_layers, num_layers_freeze, use_deepspeed, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;34mf\"Loading text from {train_data} with generation length of {block_size}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             )\n\u001b[0;32m--> 633\u001b[0;31m             train_data = TokenDataset(\n\u001b[0m\u001b[1;32m    634\u001b[0m                 \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mbos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/aitextgen/TokenDataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, vocab_file, merges_file, tokenizer, tokenizer_file, texts, line_by_line, from_cache, header, save_cache, cache_destination, compress, block_size, tokenized_texts, text_delim, bos_token, eos_token, unk_token, pad_token, progress_bar_refresh_rate, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             self.tokens = encode_tokens_from_file(\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0meos_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/aitextgen/TokenDataset.py\u001b[0m in \u001b[0;36mencode_tokens_from_file\u001b[0;34m(file_path, eos_token, tokenizer, newline, header, progress_bar_refresh_rate, batch_size)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_csv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 batch = [\n\u001b[0m\u001b[1;32m    311\u001b[0m                     \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/aitextgen/TokenDataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_csv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 batch = [\n\u001b[0;32m--> 311\u001b[0;31m                     \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 ]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "# The name of the downloaded Shakespeare text for training\n",
    "file_name = \"recipe.csv\"\n",
    "\n",
    "# Train a custom BPE Tokenizer on the downloaded text\n",
    "# This will save one file: `aitextgen.tokenizer.json`, which contains the\n",
    "# information needed to rebuild the tokenizer.\n",
    "train_tokenizer(file_name)\n",
    "tokenizer_file = \"aitextgen.tokenizer.json\"\n",
    "\n",
    "# GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training\n",
    "# e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2.\n",
    "config = GPT2ConfigCPU()\n",
    "\n",
    "# Instantiate aitextgen using the created tokenizer and config\n",
    "ai = aitextgen(tokenizer_file=tokenizer_file, config=config)\n",
    "\n",
    "# You can build datasets for training by creating TokenDatasets,\n",
    "# which automatically processes the dataset with the appropriate size.\n",
    "data = TokenDataset(file_name, tokenizer_file=tokenizer_file, block_size=64)\n",
    "\n",
    "# Train the model! It will save pytorch_model.bin periodically and after completion to the `trained_model` folder.\n",
    "# On a 2020 8-core iMac, this took ~25 minutes to run.\n",
    "#ai.train(data, batch_size=8, num_steps=50000, generate_every=5000, save_every=5000)\n",
    "ai.train(file_name, line_by_line=True, from_cache=False, num_steps=50000, generate_every=5000, save_every=5000)\n",
    "# Generate text from it!\n",
    "#ai.generate(10, prompt=\"ROMEO:\")\n",
    "\n",
    "# With your trained model, you can reload the model at any time by\n",
    "# providing the folder containing the pytorch_model.bin model weights + the config, and providing the tokenizer.\n",
    "#ai2 = aitextgen(model_folder=\"trained_model\", tokenizer_file=\"aitextgen.tokenizer.json\")\n",
    "\n",
    "#ai2.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin already exists in /trained_model and will be overwritten!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae4421b04af4b70983ea550f3988c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": 35,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      " until smooth and the mixture dist for 1 hour, or until the dough is sitled through.\n",
      "3. Slice the frosting in half and let the refrigerator for another 2 hours, if still the bottom is ready to use. Dip a mat\n",
      "==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m10,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "1 cup milk\n",
      "1 cup water\n",
      "1 lb banana\n",
      "1 tablespoon lemon juice\n",
      "1 ½ cups milk\n",
      "1 ½ cups broccoli\n",
      "1 ½ cups all purpose flour\n",
      "1 ¼ teaspoons baking powder\n",
      "½ teaspoon kosher salt\n",
      "½ teaspoon ground black pepper\n",
      "½ teaspoon ground\n",
      "==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m15,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "1 medium wheat flour\n",
      "½ teaspoon kosher salt\n",
      "4 tablespoons unsalted butter\n",
      "1 ¼ cups powdered sugar\n",
      "1 ½ tablespoons vanilla extract\n",
      "1 teaspoon large egg\n",
      "1 teaspoon vanilla extract\n",
      "\n",
      "\n",
      "1. Whisk together the eggs, eggs, milk, cinnamon, and milk until smooth.\n",
      "2\n",
      "==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m20,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "3 tablespoons yellow meinaise\n",
      "3 tablespoons cauliflower juice\n",
      "½ cup rice\n",
      "½ cup pesto\n",
      "2 tablespoons honey\n",
      "1 teaspoon garlic powder\n",
      "1 teaspoon salt\n",
      "1 teaspoon pepper\n",
      "½ teaspoon cumin\n",
      "1 teaspoon coriander powder\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m25,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      ".\n",
      "5. Add 1 of the cereala flour mixture, and salt, mixing condensed milk, for a tight.\n",
      "6. Transfer the mixture to a bowl and mix thoroughly.\n",
      "7. Add in the butter and whiskey.\n",
      "8\n",
      "==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m30,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      ", and salt. Stir in the microwave for 30 minutes, or until just tender.\n",
      "3. Serve immediately with your favorite, sprinklings.\n",
      "4. Enjoy!\n",
      "Frozen And Cream Cinnamon Rolid\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m35,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      ".\n",
      "4. Add the butter and mix thoroughly, then spread the drained crushed tomatoes, then drain the beef on paper towels, then add the seasoning, salt, pepper, and garlic powder. Toss in the pan and simmer until the sauce is\n",
      "==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m40,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      " and them just begins to get the glass.\n",
      "6. Cover the pot with plastic wrap and place in the refrigerator for at least 30 minutes. The cake will keep warm before you want at least the touch, about\n",
      "==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m45,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      ": \n",
      "2 teaspoons salt\n",
      "2 teaspoons pepper\n",
      "1 teaspoon dried oregano\n",
      "1 teaspoon salt\n",
      "1 teaspoon oregano\n",
      "½ teaspoon pepper\n",
      "½ teaspoon garlic powder\n",
      "¼ teaspoon salt\n",
      "\n",
      "1. Preheat oven to 325°F (160°C).\n",
      "2\n",
      "==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m50,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      " whey will keep in an airtight container for up to 1 months.\n",
      "3. Let the chicken rest for 10 minutes.\n",
      "4. In a large bowl, combine the flour, sugar, brown sugar, cinnamon, and salt. Stir until combined\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "file_name = \"recipe.csv\"\n",
    "\n",
    "ai.train(data, batch_size=8, num_steps=50000, generate_every=5000, save_every=5000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Important parameters for train():\n",
    "\n",
    "#line_by_line: Set this to True if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
    "#from_cache: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to True.\n",
    "#num_steps: Number of steps to train the model for.\n",
    "#generate_every: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
    "#save_every: Interval of steps to save the model: the model will be saved in the VM to /trained_model.\n",
    "#save_gdrive: Set this to True to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
    "#fp16: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
    "#Here are other important parameters for train() that are useful but you likely do not need to change.\n",
    "\n",
    "#learning_rate: Learning rate of the model training.\n",
    "#batch_size: Batch size of the model training; setting it too high will cause the GPU to go BOOM. (if using fp16, you can increase the batch size more safely)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "CUDA is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a22d0d39459c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maitextgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maitextgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# Sets the model were using to the model we just trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maitextgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"trained_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/aitextgen/aitextgen.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, model_folder, config, vocab_file, merges_file, tokenizer_file, schema_tokens, schema_return, cache_dir, tf_gpt2, to_gpu, to_fp16, verbose, gradient_checkpointing, bos_token, eos_token, unk_token, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 )\n\u001b[1;32m    273\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     def generate(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/aitextgen/aitextgen.py\u001b[0m in \u001b[0;36mto_gpu\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;34m\"\"\"Moves the model to the specified GPU.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CUDA is not installed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: CUDA is not installed."
     ]
    }
   ],
   "source": [
    "#Loading a model\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "config = GPT2ConfigCPU()\n",
    "ai = aitextgen(config=config)\n",
    "\n",
    "# Sets the model were using to the model we just trained\n",
    "ai = aitextgen(model_folder=\"trained_model\")\n",
    "\n",
    "ai.generate()\n",
    "#Call the perfect model to run\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "name": "python3-ubuntu",
   "resource_dir": "/usr/local/share/jupyter/kernels/python3-ubuntu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}