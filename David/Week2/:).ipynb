{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this post I'm going to explain how to build a simple app that will be able to use the Google Maps API to make and display your location.\n",
      "\n",
      "The first step is to create a new app. You can create one by clicking on the Edit button on the bottom of the page.\n",
      "\n",
      "Open Google Maps on your browser and click on the Set App button.\n",
      "\n",
      "From the Edit menu, select the category you want to include in your app.\n",
      "\n",
      "Once you have selected category, click on the Edit button on the top right of your page.\n",
      "\n",
      "In the Edit menu, choose the category you want to add in your app.\n",
      "\n",
      "In the Edit menu, select the Category you want to add in your app.\n",
      "\n",
      "From the Edit menu, select the category you want to add in your app.\n",
      "\n",
      "Click on the Set App button, and then click on the Set Navigation button.\n",
      "\n",
      "From the Edit menu, choose the Category you want to include in your app.\n",
      "\n",
      "You should now see the navigation bar appear.\n",
      "\n",
      "Now add the app to your Google Maps location database.\n",
      "\n",
      "You should see a new app listing for your location.\n",
      "\n",
      "Go back to the Google Maps app, and click on\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few days ago, I posted on a talk I had with a former friend of mine who is now very busy and has been doing a lot of research on the internet for the past few days. As I was talking with him, I was struck by the fact that he had been looking at information that he had recently discovered about the internet. He said that he had found a link to a series of websites that claimed they were based on the same scientific information. He started to read them\n",
      "==========\n",
      "In the wake of the tragic shooting of Michael Brown, who was unarmed at the start of this year's Ferguson, Mo., unrest, a few weeks ago, the Ferguson City Council, without any public comment, approved a police proposal to build a pedestrian bridge over the river.\n",
      "\n",
      "The plan, which the city council voted on Monday, would connect with the existing Killeen Bridge over the Jefferson, which runs through the city.\n",
      "\n",
      "The proposal, which the council approved the day before\n",
      "==========\n",
      "1.5.1 - Fixed some typos in the documentation.\n",
      "\n",
      "1.5.0 - Fixed a typo in the documentation.\n",
      "\n",
      "1.4.0 - Fixed a bug with the last line of the first sentence.\n",
      "\n",
      "1.3.1 - Fixed a typo in the documentation.\n",
      "\n",
      "1.3.0 - Fixed a typo in the documentation.\n",
      "\n",
      "1.2.0 - Fixed a typo in the documentation.\n",
      "\n",
      "1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mI believe in unicorns because\u001b[0m their numbers are so low in this area. I like to think unicorns are more like a person than a thing. I'm not sure that it's just me. It just seems like a good thing to be able to say the right things.\n",
      "\n",
      "I agree with you as well. I wouldn't mind being a part of something like the internet, but I'm not sure if I'd like to hear your opinions. So, I've got a question.\n",
      "==========\n",
      "\u001b[1mI believe in unicorns because\u001b[0m they're the way they are.\"\n",
      "\n",
      "\"Maybe I'm just a little too self-centered and I don't know how to deal with them because I've been in love with unicorns for a long time. Maybe I'm just too self-centered and I don't know how to deal with them because I've been in love with unicorns for a long time. Maybe I'm just too self-centered and I don't know how to deal with them\n",
      "==========\n",
      "\u001b[1mI believe in unicorns because\u001b[0m people are so fascinated by unicorns.\"\n",
      "\n",
      "The original article on The Daily Beast ran May 6, 2014. The article has been updated to reflect the fact that the author is an ordained minister of the Roman Catholic church.\n",
      "\n",
      "The author, who is Catholic, says he's been inspired by the writings of the late Roman Catholic bishop John Paul II.\n",
      "\n",
      "\"I found my voice in the pages of the Holy Bible when I was growing up in Rome,\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153e7ab4b1eb4a1aaa54f736be854a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": 2,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "# Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model\n",
    "ai = aitextgen()\n",
    "\n",
    "ai.generate()\n",
    "ai.generate(n=3, max_length=100)\n",
    "ai.generate(n=3, prompt=\"I believe in unicorns because\", max_length=100)\n",
    "ai.generate_to_file(n=10, prompt=\"I believe in unicorns because\", max_length=100, temperature=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are tokenizing a CSV file, but you did not set line_by_line=True. Please change if unintended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53669e6b891045ada2e660cc3583d9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4961 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e48c9a1e844fe5bec2d2d4b2949043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128055 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m data \u001b[38;5;241m=\u001b[39m TokenDataset(file_name, tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file, block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Train the model! It will save pytorch_model.bin periodically and after completion to the `trained_model` folder.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# On a 2020 8-core iMac, this took ~25 minutes to run.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#ai.train(data, batch_size=8, num_steps=50000, generate_every=5000, save_every=5000)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_by_line\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/aitextgen/aitextgen.py:633\u001b[0m, in \u001b[0;36maitextgen.train\u001b[0;34m(self, train_data, output_dir, fp16, fp16_opt_level, n_gpu, tpu_cores, max_grad_norm, gradient_accumulation_steps, seed, learning_rate, weight_decay, adam_epsilon, warmup_steps, num_steps, save_every, generate_every, n_generate, loggers, batch_size, num_workers, benchmark, avg_loss_smoothing, save_gdrive, run_id, progress_bar_refresh_rate, freeze_layers, num_layers_freeze, use_deepspeed, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m     block_size \u001b[38;5;241m=\u001b[39m model_max_length(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m    630\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading text from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with generation length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m     )\n\u001b[0;32m--> 633\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mTokenDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline_by_line\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_data\u001b[38;5;241m.\u001b[39mline_by_line)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m freeze_layers \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_tf_gpt2 \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1558M\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/aitextgen/TokenDataset.py:171\u001b[0m, in \u001b[0;36mTokenDataset.__init__\u001b[0;34m(self, file_path, vocab_file, merges_file, tokenizer, tokenizer_file, texts, line_by_line, from_cache, header, save_cache, cache_destination, compress, block_size, tokenized_texts, text_delim, bos_token, eos_token, unk_token, pad_token, progress_bar_refresh_rate, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens \u001b[38;5;241m=\u001b[39m encode_tokens_from_list(\n\u001b[1;32m    168\u001b[0m         texts, eos_token, tokenizer, progress_bar_refresh_rate\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens \u001b[38;5;241m=\u001b[39m \u001b[43mencode_tokens_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_delim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar_refresh_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m block_size\n\u001b[1;32m    182\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are fewer than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m encoded tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_subsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m block_size\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/aitextgen/TokenDataset.py:310\u001b[0m, in \u001b[0;36mencode_tokens_from_file\u001b[0;34m(file_path, eos_token, tokenizer, newline, header, progress_bar_refresh_rate, batch_size)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_csv:\n\u001b[0;32m--> 310\u001b[0m         batch \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m             text[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m eos_token\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(f_read, batch_size))\n\u001b[1;32m    313\u001b[0m         ]\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m         batch \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    316\u001b[0m             text \u001b[38;5;241m+\u001b[39m eos_token\n\u001b[1;32m    317\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(f_read, batch_size))\n\u001b[1;32m    318\u001b[0m         ]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/aitextgen/TokenDataset.py:311\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_csv:\n\u001b[1;32m    310\u001b[0m         batch \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 311\u001b[0m             \u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m eos_token\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(f_read, batch_size))\n\u001b[1;32m    313\u001b[0m         ]\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m         batch \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    316\u001b[0m             text \u001b[38;5;241m+\u001b[39m eos_token\n\u001b[1;32m    317\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(f_read, batch_size))\n\u001b[1;32m    318\u001b[0m         ]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "# The name of the downloaded Shakespeare text for training\n",
    "file_name = \"recipe.csv\"\n",
    "\n",
    "# Train a custom BPE Tokenizer on the downloaded text\n",
    "# This will save one file: `aitextgen.tokenizer.json`, which contains the\n",
    "# information needed to rebuild the tokenizer.\n",
    "train_tokenizer(file_name)\n",
    "tokenizer_file = \"aitextgen.tokenizer.json\"\n",
    "\n",
    "# GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training\n",
    "# e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2.\n",
    "config = GPT2ConfigCPU()\n",
    "\n",
    "# Instantiate aitextgen using the created tokenizer and config\n",
    "ai = aitextgen(tokenizer_file=tokenizer_file, config=config)\n",
    "\n",
    "# You can build datasets for training by creating TokenDatasets,\n",
    "# which automatically processes the dataset with the appropriate size.\n",
    "data = TokenDataset(file_name, tokenizer_file=tokenizer_file, block_size=64)\n",
    "\n",
    "# Train the model! It will save pytorch_model.bin periodically and after completion to the `trained_model` folder.\n",
    "# On a 2020 8-core iMac, this took ~25 minutes to run.\n",
    "#ai.train(data, batch_size=8, num_steps=50000, generate_every=5000, save_every=5000)\n",
    "ai.train(file_name, line_by_line=True, from_cache=False, num_steps=50000, generate_every=5000, save_every=5000)\n",
    "# Generate text from it!\n",
    "#ai.generate(10, prompt=\"ROMEO:\")\n",
    "\n",
    "# With your trained model, you can reload the model at any time by\n",
    "# providing the folder containing the pytorch_model.bin model weights + the config, and providing the tokenizer.\n",
    "#ai2 = aitextgen(model_folder=\"trained_model\", tokenizer_file=\"aitextgen.tokenizer.json\")\n",
    "\n",
    "#ai2.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maitextgen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aitextgen\n\u001b[1;32m      6\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecipe.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mai\u001b[49m\u001b[38;5;241m.\u001b[39mtrain(file_name, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, num_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, generate_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, save_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ai' is not defined"
     ]
    }
   ],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "file_name = \"recipe.csv\"\n",
    "\n",
    "ai.train(file_name, batch_size=8, num_steps=50000, generate_every=5000, save_every=5000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Important parameters for train():\n",
    "\n",
    "#line_by_line: Set this to True if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
    "#from_cache: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to True.\n",
    "#num_steps: Number of steps to train the model for.\n",
    "#generate_every: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
    "#save_every: Interval of steps to save the model: the model will be saved in the VM to /trained_model.\n",
    "#save_gdrive: Set this to True to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
    "#fp16: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
    "#Here are other important parameters for train() that are useful but you likely do not need to change.\n",
    "\n",
    "#learning_rate: Learning rate of the model training.\n",
    "#batch_size: Batch size of the model training; setting it too high will cause the GPU to go BOOM. (if using fp16, you can increase the batch size more safely)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!/\u000b3/ally\"\u000b2 th o though\u000b2 th,\"ore\u000b3and than spe\u000b2 th oaryc000;\u000b3 r partren than\u000bur thip Tr\u000burowkscom\u000b2ow partall F\u000bTq shouldout\u000b2ow ro The\u000b\n"
     ]
    }
   ],
   "source": [
    "#Loading a model\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "config = GPT2ConfigCPU()\n",
    "ai = aitextgen(config=config)\n",
    "\n",
    "# Sets the model were using to the model we just trained\n",
    "ai = aitextgen(model_folder=\"trained_model\")\n",
    "\n",
    "ai.generate()\n",
    "#Call the perfect model to run\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "nlp_env",
   "resource_dir": "/projects/b9e0cba5-06d0-4604-824f-bbb5d6223324/.local/share/jupyter/kernels/nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}